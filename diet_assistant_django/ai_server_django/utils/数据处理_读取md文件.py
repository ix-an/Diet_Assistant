"""
RAGçŸ¥è¯†åº“é¢„å¤„ç†æ¨¡å—ï¼šå°†Markdownæ–‡æ¡£æ¸…æ´—ã€è¯­ä¹‰åˆ†å‰²ã€åˆå¹¶åç”Ÿæˆç»“æ„åŒ–çŸ¥è¯†å—
æ ¸å¿ƒæµç¨‹ï¼šæ ¼å¼æ¸…æ´— â†’ è¯­ä¹‰åˆ†å‰² â†’ åˆ†å—ä¼˜åŒ– â†’ JSONæŒä¹…åŒ–
é€‚ç”¨åœºæ™¯ï¼šæ„å»ºé«˜è´¨é‡RAGçŸ¥è¯†åº“å‰çš„æ•°æ®é¢„å¤„ç†
"""

import json
import re
from pathlib import Path
from modelscope.outputs import OutputKeys
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks


class MarkdownRAGProcessor:
    """
    Markdownæ–‡æ¡£RAGé¢„å¤„ç†å™¨
    åŠŸèƒ½ï¼šæ¸…æ´—Markdownå™ªå£° â†’ è°ƒç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹ â†’ åˆå¹¶çŸ­æ–‡æœ¬å— â†’ ç”Ÿæˆæ ‡å‡†åŒ–çŸ¥è¯†åº“JSON
    """

    def __init__(self, model_path="./segmentation-models", min_chunk_length=120):
        """
        åˆå§‹åŒ–å¤„ç†å™¨

        Args:
            model_path (str): ModelScopeæ–‡æ¡£åˆ†å‰²æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆéœ€æå‰ä¸‹è½½ï¼‰
            min_chunk_length (int): åˆå¹¶åå•ä¸ªçŸ¥è¯†å—çš„æœ€å°å­—ç¬¦é•¿åº¦ï¼ˆé˜²ç¢ç‰‡åŒ–ï¼‰
        """
        # åŠ è½½ModelScopeæ–‡æ¡£è¯­ä¹‰åˆ†å‰²pipelineï¼ˆæ”¯æŒä¸­æ–‡æ–‡æ¡£ç»“æ„ç†è§£ï¼‰
        self.pipeline = pipeline(
            task=Tasks.document_segmentation,  # ä»»åŠ¡ç±»å‹ï¼šæ–‡æ¡£åˆ†å‰²
            model=model_path,  # æ¨¡å‹è·¯å¾„
            model_revision="master",  # æ¨¡å‹ç‰ˆæœ¬
        )
        self.min_chunk_length = min_chunk_length

        # å ä½ç¬¦è®¾è®¡è¯´æ˜ï¼š
        # - ç§»é™¤å¥å·ä¿æŠ¤ï¼ˆ__DOT__ï¼‰ï¼šä¿ç•™åŸå§‹æ ‡ç‚¹åˆ©äºæ¨¡å‹è¯†åˆ«è¯­ä¹‰è¾¹ç•Œ
        # - ä»…ä¿æŠ¤æ¢è¡Œç¬¦ï¼šé¿å…pipelineå°†æ¢è¡Œè¯¯åˆ¤ä¸ºåˆ†å‰²ç‚¹å¯¼è‡´è¯­ä¹‰æ–­è£‚
        self.newline_placeholder = "__NEWLINE__"

    def _clean_markdown(self, text):
        """
        æ¸…æ´—Markdownå™ªå£°å…ƒç´ ï¼ˆä¿ç•™è¯­ä¹‰ç»“æ„ï¼Œç§»é™¤å¹²æ‰°é¡¹ï¼‰

        å¤„ç†é€»è¾‘ï¼š
        1. ç§»é™¤å›¾ç‰‡é“¾æ¥ï¼ˆå«images/è·¯å¾„çš„ç‰¹æ®Šæ ¼å¼ï¼‰
        2. æ¸…é™¤æ°´å¹³åˆ†å‰²çº¿ï¼ˆ---ï¼‰
        3. è§„èŒƒåŒ–ç©ºæ ¼ï¼ˆè¿ç»­ç©ºæ ¼â†’å•ç©ºæ ¼ï¼‰
        4. æ¸…ç†è¡¨æ ¼å±æ€§ä¸­çš„å¼•å·ï¼ˆå¦‚ align="center" â†’ align=centerï¼‰

        Args:
            text (str): åŸå§‹Markdownæ–‡æœ¬
        Returns:
            str: æ¸…æ´—åçš„çº¯æ–‡æœ¬ï¼ˆä¿ç•™æ ‡é¢˜/åˆ—è¡¨/è¡¨æ ¼ç»“æ„ç¬¦å·ï¼‰
        """
        # ç§»é™¤æ ‡å‡†Markdownå›¾ç‰‡è¯­æ³• ![alt](url)
        text = re.sub(r"!\[.*?\]\(.*?\)", "", text)
        # ç§»é™¤ç‰¹å®šè·¯å¾„å›¾ç‰‡ï¼ˆå¦‚æœ¬åœ°å­˜å‚¨çš„images/ç›®å½•å›¾ç‰‡ï¼‰
        text = re.sub(r"\[.*?\]\(images/.*?\)", "", text)
        # åˆ é™¤ç‹¬ç«‹è¡Œçš„æ°´å¹³åˆ†å‰²çº¿ï¼ˆ3ä¸ªä»¥ä¸Š-ä¸”å‰åæ— å…¶ä»–å­—ç¬¦ï¼‰
        text = re.sub(r"^-{3,}\s*$", "", text, flags=re.MULTILINE)
        # åˆå¹¶è¿ç»­ç©ºæ ¼ä¸ºå•ç©ºæ ¼
        text = re.sub(r" +", " ", text)
        # æ¸…ç†è¡¨æ ¼HTMLå±æ€§ä¸­çš„å¼•å·ï¼ˆé¿å…JSONåºåˆ—åŒ–å†²çªï¼‰
        text = re.sub(r'(\w+)="([^"]*)"', r"\1=\2", text)
        text = re.sub(r"(\w+)='([^']*)'", r"\1=\2", text)
        return text.strip()

    def _protect_text(self, text):
        """
        ä¿æŠ¤æ¢è¡Œç¬¦ï¼šæ›¿æ¢ä¸ºå ä½ç¬¦é˜²æ­¢pipelineè¯¯åˆ†å‰²

        è®¾è®¡åŸå› ï¼š
        - æ–‡æ¡£åˆ†å‰²æ¨¡å‹å¯èƒ½å°†æ¢è¡Œç¬¦è§†ä¸ºå¼ºåˆ†å‰²ç‚¹
        - ç”¨å ä½ç¬¦ä¸´æ—¶ä¿ç•™æ®µè½è¾¹ç•Œä¿¡æ¯
        - æ³¨æ„ï¼šä¸ä¿æŠ¤å¥å·ï¼ˆä¿ç•™æ ‡ç‚¹åˆ©äºè¯­ä¹‰åˆ†å‰²æ¨¡å‹åˆ¤æ–­å¥å­ç»“æŸï¼‰

        Args:
            text (str): åŸå§‹æ–‡æœ¬
        Returns:
            str: æ¢è¡Œç¬¦è¢«æ›¿æ¢çš„æ–‡æœ¬
        """
        return text.replace("\n", self.newline_placeholder)

    def _restore_text(self, text):
        """
        æ¢å¤æ–‡æœ¬ï¼šå°†å ä½ç¬¦è½¬ä¸ºç©ºæ ¼ï¼ˆéæ¢è¡Œç¬¦ï¼ï¼‰

        å…³é”®è®¾è®¡ï¼š
        - æ¢å¤ä¸º" "è€Œé"\n"ï¼šç¡®ä¿åˆ†å‰²åçš„æ–‡æœ¬å—ä¸ºè¿ç»­æ®µè½
        - é¿å…åç»­åµŒå…¥æ¨¡å‹å°†æ¢è¡Œç¬¦è§†ä¸ºç‰¹æ®Štokenå¹²æ‰°è¯­ä¹‰
        - ä¸_clean_markdowné…åˆå®ç°"ç»“æ„ä¿ç•™+å™ªå£°æ¸…é™¤"å¹³è¡¡

        Args:
            text (str): å«å ä½ç¬¦çš„æ–‡æœ¬
        Returns:
            str: æ¢å¤ä¸ºç©ºæ ¼åˆ†éš”çš„è¿ç»­æ–‡æœ¬
        """
        return text.replace(self.newline_placeholder, " ")

    def merge_chunks(self, chunks):
        """
        æ™ºèƒ½åˆå¹¶çŸ­æ–‡æœ¬å—ï¼ˆè§£å†³æ¨¡å‹åˆ†å‰²è¿‡ç¢é—®é¢˜ï¼‰

        ç­–ç•¥ï¼š
        1. è·³è¿‡ç©ºå—
        2. é¡ºåºåˆå¹¶ç›´åˆ°è¾¾åˆ°min_chunk_length
        3. æœ«å°¾å‰©ä½™çŸ­å—è¿½åŠ åˆ°å‰ä¸€å—ï¼ˆé¿å…å­¤ç«‹çŸ­å¥ï¼‰

        Args:
            chunks (List[str]): åŸå§‹åˆ†å‰²æ–‡æœ¬å—åˆ—è¡¨
        Returns:
            List[str]: åˆå¹¶ä¼˜åŒ–åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        merged = []
        current = ""
        for chunk in chunks:
            if not chunk.strip():  # è·³è¿‡ç©ºå—
                continue
            # ç´¯ç§¯å½“å‰å—
            current = f"{current} {chunk}" if current else chunk

            # è¾¾åˆ°æœ€å°é•¿åº¦åˆ™å­˜å…¥ç»“æœ
            if len(current) >= self.min_chunk_length:
                merged.append(current.strip())
                current = ""

        # å¤„ç†å‰©ä½™å†…å®¹ï¼šè¿½åŠ åˆ°æœ€åä¸€å—ï¼ˆé¿å…äº§ç”Ÿè¶…çŸ­å°¾å—ï¼‰
        if current:
            if merged:
                merged[-1] += " " + current.strip()
            else:  # ç‰¹æ®Šæƒ…å†µï¼šå…¨æ–‡å‡çŸ­äºé˜ˆå€¼
                merged.append(current.strip())
        return merged

    def process_files(self, input_dir, output_file):
        """
        æ‰¹é‡å¤„ç†Markdownæ–‡ä»¶å¹¶ç”ŸæˆçŸ¥è¯†åº“JSON

        æµç¨‹ï¼š
        1. éå†ç›®å½•ä¸‹æ‰€æœ‰.mdæ–‡ä»¶
        2. å•æ–‡ä»¶å¤„ç†ï¼šæ¸…æ´—â†’ä¿æŠ¤â†’è¯­ä¹‰åˆ†å‰²â†’æ¢å¤â†’äºŒæ¬¡æ¸…æ´—â†’åˆå¹¶
        3. æ„å»ºå¸¦å…ƒæ•°æ®çš„çŸ¥è¯†å—
        4. æŒä¹…åŒ–ä¸ºæ ‡å‡†JSON

        Args:
            input_dir (str): Markdownæºæ–‡ä»¶ç›®å½•è·¯å¾„
            output_file (str): è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        """
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ é”™è¯¯ï¼šç›®å½• {input_dir} ä¸å­˜åœ¨")
            return

        knowledges = []  # å­˜å‚¨æ‰€æœ‰çŸ¥è¯†å—
        files = list(input_path.glob("*.md"))

        print(f"ğŸ“ å¼€å§‹å¤„ç† {len(files)} ä¸ªMarkdownæ–‡ä»¶...")

        for file_path in files:
            try:
                # ============ æ­¥éª¤1ï¼šè¯»å–åŸå§‹Markdown ============
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # ============ æ­¥éª¤2ï¼šé¢„å¤„ç†ä¸è¯­ä¹‰åˆ†å‰² ============
                # 2.1 ä¿æŠ¤æ¢è¡Œç¬¦ï¼ˆé˜²pipelineè¯¯åˆ†å‰²ï¼‰
                protected = self._protect_text(content)
                # 2.2 è°ƒç”¨ModelScopeæ–‡æ¡£åˆ†å‰²æ¨¡å‹ï¼ˆæ ¸å¿ƒï¼šæŒ‰è¯­ä¹‰åˆ‡åˆ†ï¼‰
                result = self.pipeline(documents=protected)
                # 2.3 è·å–åˆ†å‰²ç»“æœï¼ˆModelScopeè¿”å›æ ¼å¼ï¼šOutputKeys.TEXTï¼‰
                raw_chunks = result[OutputKeys.TEXT].strip().split("\n")

                # ============ æ­¥éª¤3ï¼šåå¤„ç†æ¯ä¸ªåˆ†å— ============
                cleaned_chunks = []
                for chunk in raw_chunks:
                    # 3.1 æ¢å¤æ–‡æœ¬ï¼ˆæ¢è¡Œç¬¦â†’ç©ºæ ¼ï¼‰
                    restored = self._restore_text(chunk)
                    # 3.2 äºŒæ¬¡æ¸…æ´—ï¼ˆç§»é™¤åˆ†å‰²åæ®‹ç•™å™ªå£°ï¼‰
                    final_text = self._clean_markdown(restored)
                    if final_text:  # ä¸¢å¼ƒç©ºå—
                        cleaned_chunks.append(final_text)

                # ============ æ­¥éª¤4ï¼šåˆå¹¶ä¼˜åŒ– ============
                merged_chunks = self.merge_chunks(cleaned_chunks)

                # ============ æ­¥éª¤5ï¼šæ„å»ºçŸ¥è¯†åº“æ¡ç›® ============
                for chunk in merged_chunks:
                    knowledges.append({
                        "metadata": {
                            "source": file_path.name,  # ä¿ç•™æ¥æºæ–‡ä»¶åï¼ˆæº¯æºå…³é”®ï¼‰
                            "department": "",  # é¢„ç•™éƒ¨é—¨å­—æ®µï¼ˆä¾¿äºåç»­æ‰©å±•ï¼‰
                        },
                        "content": chunk,  # æ¸…æ´—åˆå¹¶åçš„æœ‰æ•ˆæ–‡æœ¬
                    })
                print(f"âœ… æˆåŠŸå¤„ç†: {file_path.name} â†’ ç”Ÿæˆ {len(merged_chunks)} ä¸ªçŸ¥è¯†å—")

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {type(e).__name__}: {e}")

        # ============ æ­¥éª¤6ï¼šæŒä¹…åŒ–è¾“å‡º ============
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        with open(output_path, "w", encoding="utf-8") as f:
            # ensure_ascii=Falseï¼šä¿ç•™ä¸­æ–‡ï¼›indent=4ï¼šç¾åŒ–æ ¼å¼ä¾¿äºäººå·¥æ£€æŸ¥
            json.dump(knowledges, f, ensure_ascii=False, indent=4)
        print(f"\nâœ¨ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å…±ç”Ÿæˆ {len(knowledges)} ä¸ªçŸ¥è¯†å—")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        print(f"ğŸ“Š çŸ¥è¯†å—ç»Ÿè®¡: æœ€å°é•¿åº¦={min(len(k['content']) for k in knowledges)} | "
              f"æœ€å¤§é•¿åº¦={max(len(k['content']) for k in knowledges)}")


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    1. ç¡®ä¿å·²ä¸‹è½½ModelScopeæ–‡æ¡£åˆ†å‰²æ¨¡å‹è‡³ ./segmentation-models
       ï¼ˆæ¨èæ¨¡å‹ï¼šdamo/nlp_bert_document-segmentation_chinese-baseï¼‰
    2. å°†å¾…å¤„ç†Markdownæ–‡ä»¶æ”¾å…¥ ./ragdatasets ç›®å½•
    3. è¿è¡Œåç”Ÿæˆ ./chunks/knowledges.json ä¾›RAGç³»ç»Ÿä½¿ç”¨
    """
    processor = MarkdownRAGProcessor(
        model_path="./segmentation-models",  # å¯æ›¿æ¢ä¸ºModelScopeæ¨¡å‹IDï¼ˆéœ€è”ç½‘ï¼‰
        min_chunk_length=120  # æ ¹æ®embeddingæ¨¡å‹è°ƒæ•´ï¼ˆå¦‚text2vecå»ºè®®100-300ï¼‰
    )
    processor.process_files(
        input_dir="./ragdatasets",
        output_file="./chunks/knowledges.json"
    )

    """
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
 	 æ¨¡å‹è¦æ±‚ï¼šéœ€æå‰ä¸‹è½½æ–‡æ¡£åˆ†å‰²æ¨¡å‹ï¼ˆModelScopeæ”¯æŒç¦»çº¿åŠ è½½ï¼‰
    """